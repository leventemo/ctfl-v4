# 1.4 Test Activities, Testware & Test Roles

**testing is context dependent, but** there are **common sets of test activities**
* without which testing is less likely to achieve *test objectives*
* these **form a test process**
  + the test process can be tailored to a given situation based on various factors
    - which test activities are included in this test process,
    - how they are implemented,
    - when they occur
    - ... is normally decided as part of the *test planning* for the specific situation

the following sections describe the **general aspects of this test process** in terms of:
* test activities & tasks,
* the impact of context,
* testware,
* traceability between the *test basis* & testware,
* testing roles

ISO/IEC/IEEE 29119-2 standard: further info about test processes

## 1.4.1 Test Activities & Tasks

a test process usually consists of the **main groups of activities** described below
* although many may **appear to follow a logical sequence**, they are **often implemented iteratively / in parallel**
* they usually need to be **tailored to the system & the project**

**test planning** consists of:
* **defining the *test objectives***,
* then **selecting an approach that** best achieves the objectives within the constraints imposed by the overall context

**test monitoring & control**
* *test monitoring* involves
  + the ongoing checking of all test activities,
  + the comparison of actual progress against the plan
* *test control* involves
  + taking the actions necessary to meet the objectives of testing

**test analysis**
* includes **analyzing the *test basis***
  + to identify testable features,
  + to define & prioritize associated *test conditions*, together with the related *risks* & *risk levels*
* the ***test basis* & the *test objects* are also evaluated**
  + to identify *defects* they may contain,
  + to assess their testability
* often supported by the use of *test techniques*
* answers the question **“what to test?”**
  + in terms of measurable coverage criteria

**test design** includes:
* **elaborating the *test conditions* into *test cases* and other *testware*** (eg. *test charters*)
  + this often involves the **identification of *coverage items***, which serve as a guide to specify *test case* inputs
  + ***test techniques*** can be used **to support** this activity
* also includes:
  + **defining the *test data*** requirements,
  + **designing the test environment** & identifying any other required infrastructure & tools
* answers the question **“how to test?”**

**test implementation**
* includes **creating / acquiring the *testware* necessary** for test execution (eg. *test data*)
* ***test cases* can be organized into *test procedures*** & are often **assembled into test suites**
* manual & automated **test scripts are created**
* ***test procedures* are prioritized & arranged within a *test execution* schedule** for efficient *test execution*
* the **test environment is built & verified**

**test execution**
* includes **running the tests** in accordance with the *test execution* schedule (test runs)
* may be **manual** / **automated**
* **can take many forms**, eg. continuous testing / pair testing sessions
* actual test **results are compared with the expected results**
* test results are **logged**
* **anomalies are analyzed** to identify their likely causes
  + this analysis allows us to **report the anomalies** based on the *failures* observed

**test completion** activities
* **usually occur at project milestones**
  + (e.g., release, end of iteration, *test level* completion)
  + for any unresolved *defects*, **change requests** / **product backlog** items created
* any **testware** that may be useful in future is **identified & archived / handed over** to the appropriate teams
* the **test environment is shut down** to an agreed state
* the **test activities are analyzed**
  + to identify lessons learned & improvements for future iterations, releases / projects
* a ***test completion report* is created & communicated** to the stakeholders

## 1.4.2 Test Process in Context

testing is not performed in isolation
* test activities are an integral part of the development processes
* testing is also funded by stakeholders & its final goal is to help fulfill the stakeholders’ business needs

so the way the testing is carried out will depend on a number of contextual factors, eg.:
* **stakeholders**
  + needs, expectations, requirements, willingness to cooperate, etc.
* **team members**
  + skills, knowledge, level of experience, availability, training needs, etc.
* **business domain**
  + criticality of the *test object*, identified *risks*, market needs, specific legal regulations, etc.
* **technical factors**
  + type of software, product architecture, technology used, etc.
* **project constraints**
  + scope, time, budget, resources, etc.
* **organizational factors**
  + organizational structure, existing policies, practices used, etc.
* **software development lifecycle**
  + engineering practices, development methods, etc.
* **tools**
  + availability, usability, compliance, etc.

these factors will have an impact on many test-related issues, eg.:
* test strategy, *test techniques* used, degree of *test automation*, required level of *coverage*, level of detail of test documentation, reporting, etc.

## 1.4.3 Testware

**testware is created as output work products** from the test activities
* a significant variation: how different organizations produce, shape, name, organize & manage their work products
* proper configuration management ensures consistency & integrity of work products

work products include:
* **test planning work products** include:
  + *test plan*,
  + test schedule,
  + *risk* register,
    - a list of *risks* together with *risk* likelihood, *risk* impact & information about *risk mitigation*
  + *entry & exit criteria*
  + the three above are often a part of the *test plan*
* **test monitoring & control work products** include:
  + *test progress reports*,
  + documentation of control directives
  + *risk* information
* **test analysis work products** include:
  + (prioritized) *test conditions*
    - eg. *acceptance criteria*
  + *defect reports* regarding *defects* in the *test basis* (if not fixed directly)
* **test design work products** include:
  + (prioritized) *test cases*,
  + *test charters*,
  + *coverage items*,
  + *test data* requirements,
  + test environment requirements
* **test implementation work products** include:
  + *test procedures*,
  + *automated test* scripts,
  + test suites,
  + *test data*,
  + test execution schedule,
  + test environment elements, eg.
    - stubs,
    - drivers,
    - simulators,
    - service virtualizations
* **test execution work products** include:
  + test logs & *defect reports*
* **test completion work products** include:
  + *test completion report*,
  + action items for improvement of subsequent projects / iterations,
  + documented lessons learned,
  + change requests (eg. as product backlog items)

## 1.4.4 Traceability between the Test Basis & Testware

for effective *test monitoring* and *control*, it is important to **establish & maintain traceability** throughout the test process
* **between**
  + the *test basis* elements,
  + *testware* associated with these elements (eg. *test conditions*, *risks*, *test cases*),
  + *test results*,
  + detected *defects*

accurate traceability supports **coverage evaluation**,
* so it is very useful if **measurable *coverage criteria*** are defined in the *test basis*
* the *coverage criteria* can **function as key performance indicators**
  + to drive the activities that show to what extent the *test objectives* have been achieved, eg.:
    - **traceability of *test cases* to requirements** can verify that the requirements are covered by *test cases*
    - **traceability of test results to *risks*** can be used to evaluate the level of residual *risk* in a *test object*

in addition to evaluating *coverage*, **good traceability also**
* makes it possible to determine the impact of changes, facilitates test audits & helps meet IT governance criteria
* also makes test progress & *completion reports* more easily understandable by including the status of *test basis* elements
  + this can also assist in communicating the technical aspects of testing to stakeholders in an understandable manner
* provides information to assess product quality, process capability & project progress against business goals

## 1.4.5 Roles in Testing

two principal roles in testing are covered here: a **test management role** & a **testing role**
* the activities & tasks assigned to these two roles **depend on** factors such as
  + the **project & product context**,
  + the **skills of the people** in the roles & **the organization**

the **test management role**
* takes **overall responsibility for the test process, test team & leadership of** the test activities
* **mainly focused on the activities of**
  + *test planning*,
  + *test monitoring & control*,
  + *test completion*
* the way in which the test management role is carried out **varies depending on the context**
  + eg. in Agile, some of the *test management* tasks may be handled by the Agile team
  + tasks that span multiple teams / the entire organization may be performed by test managers outside of the development team

the **testing role**
* takes **overall responsibility for the engineering** (technical) aspect of testing
* is **mainly focused on the activities of**
  + *test analysis*,
  + *test design*,
  + *test implementation*,
  + *test execution*

**different people may take on these roles at different times**:
* eg. the test management role can be performed by a team leader, by a test manager, by a development manager, etc.
* it is also possible for one person to take on the roles of testing & *test management* at the same time
