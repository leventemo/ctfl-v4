# 2.2 Test Levels & Test Types

**test levels** def.
* **groups of test activities** that are **organized & managed together**
* each one **an instance of the test process**,
  + performed in relation to software **at a given stage** of dev,
  + from **individual components** to **complete systems**
    - or, where applicable, **systems of systems**
* they are **related to other activities within the SDLC**
  + **in sequential** SDLC models:
    - often defined such that the ***exit criteria* of one level** are **part of the *entry criteria* for the next**
  + **in some iterative** models:
    - this may not apply
* dev activities **may span through multiple *test levels*** & ***test levels* may overlap** in time

**test types** def.
* **groups of test activities related to specific quality characteristics**
* *most* of those test activities **can be performed at every *test level***

## 2.2.1 Test Levels

in this syllabus, the following five test levels are described:

***component testing*** / unit testing
* focuses on **testing components in isolation**
* often requires **specific support**
  + **test harnesses** or **unit test frameworks**
* **normally performed by devs** in their dev environments

***component integration testing*** / unit integration testing
* focuses on **testing the interfaces & interactions between components**
* heavily **dependent on the integration strategy approaches**
  + like bottom-up, top-down or big-bang

***system testing***
* focuses on the **overall behavior & capabilities of an entire system** or product,
  + **often including**
    - *functional testing* of end-to-end tasks,
    - the *non-functional testing* of quality characteristics
  + for **some non-functional quality characteristics**,
    - it is preferable to test them on a complete system in a representative test environment (eg. usability)
    - using simulations of sub-systems is also possible
* **may be performed by an independent** test team
* is **related to specifications** for the system

***system integration testing***
* focuses on **testing the interfaces of the system under test & other systems**
* **requires suitable test environments**
  + preferably similar to the operational environment

***acceptance testing***
* focuses on **validation & on demonstrating readiness for deployment**,
  + which means that the system fulfills the user’s business needs
* ideally, **should be performed by the intended users**
* **main forms of acceptance testing**:
  + user *acceptance testing* (UAT),
  + operational *acceptance testing*,
  + contractual & regulatory *acceptance testing*,
  + alpha testing & beta testing

*test levels* are **distinguished by the following non-exhaustive list of attributes**,
* (to avoid overlapping of test activities):
* *test object*
* *test objectives*
* *test basis*
* *defects* & *failures*
* approach & responsibilities

## 2.2.2 Test Types

a lot of test types exist & can be applied in projects
* in this syllabus, the following four test types are addressed:

***functional testing***
* **evaluates the functions** that a component / system should perform
  + the functions are **“what”** the test object should do
* the **main objective** is checking:
  + functional completeness,
  + functional correctness,
  + functional appropriateness

***non-functional testing***
* **evaluates attributes other than functional characteristics** of a component / system
* it is the testing of **“how well the system behaves”**
* the **main objective** is checking the non-functional software quality characteristics
  + (see: ISO/IEC 25010 standard):
  + performance efficiency
  + compatibility
  + usability
  + reliability
  + security
  + maintainability
  + portability
* it is **sometimes appropriate** for *non-functional testing* **to start early** in the life cycle
  + eg. as part of *reviews* & *component testing* / *system testing*
  + late discovery of non-functional *defects* –> a serious threat to the success of a project
* **many** non-functional tests are **derived from functional tests**
  + they use the same functional tests, but check if, while performing the function, a non-functional constraint is satisfied
    - eg. checking that a function performs within a specified time,
    - or a function can be ported to a new platform
* it **sometimes** needs a **very specific test environment**,
  + such as a usability lab for usability testing

**black-box testing**
* is **specification-based** & **derives tests from documentation external to** the *test object*
* **main objective**: checking the system's behavior against its specifications

**white-box testing**
* is **structure-based** & **derives tests from the system's implementation / internal structure**
  + eg. code, architecture, work flows, & data flows
* **main objective**: to cover the underlying structure by the tests to the acceptable level

**all four** *test types* **can be applied to all *test levels***,
* although the **focus will be different** at each level
* different *test techniques* can be used
  + to derive *test conditions* & *test cases* for all the mentioned *test types*

## 2.2.3 Confirmation Testing & Regression Testing

**changes** are typically made to a component / system
* to either **enhance** it by adding a new feature
* or to **fix** it by removing a *defect*
* testing should then also include *confirmation testing* & *regression testing*

***confirmation testing***
* **confirms** that an original *defect* has been successfully **fixed**
* depending on the *risk*, a fix can be tested **in several ways**, eg.:
  + **executing all test cases that previously have failed** due to the *defect*,
  + **by adding new tests to cover any changes** that were needed to fix the *defect*
* however, **when time / money is short** when fixing *defects*,
  + it might be restricted to simply exercising the steps that should **reproduce the *failure*** caused by the *defect*
  + **and checking** that the *failure* does not occur

***regression testing***
* **confirms that no adverse consequences have been caused by a change**,
  + **including** a fix that has already been *confirmation tested*
* these **adverse consequences could affect**
  + **the same component** where the change was made,
  + **other components** in the same system,
  + or **even other connected systems**
* it may not be restricted to the *test object* itself but **can also be related to the environment**
* it is advisable first to perform an **impact analysis**
  + **to optimize the extent** of the *regression testing*
  + impact analysis shows which parts of the software could be affected
* *regression test* suites are **run many times**
  + and generally the **number** of regression *test cases* **will increase with each iteration or release**
* so *regression testing* is a **strong candidate for automation**
* **automation** of these tests **should start early** in the project
* **where CI is used**, such as in DevOps,
  + it is good practice to **also include automated regression tests**
  + depending on the situation, **this may include regression tests on different levels**

*confirmation testing* and/or *regression testing* for the *test object* are **needed on all *test levels***
* if *defects* are fixed and/or changes are made on these *test levels*
