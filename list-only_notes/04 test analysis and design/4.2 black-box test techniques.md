# 4.2 Black-Box Test Techniques

commonly used *black-box test techniques* discussed here:
* *equivalence partitioning*
* *boundary value analysis*
* *decision table testing*
* *state transition testing*

## 4.2.1 Equivalence Partitioning

***equivalence partitioning***, aka EP
* **divides *data* into *partitions*** aka *equivalence partitions*
* **based on the expectation** that
  + all the elements of a given *partition* are to be processed in the same way by the *test object*
* **theory behind** this technique:
  + if a *test case*, which tests one value from an *equivalence partition*, detects a *defect*,
  + this *defect* should also be detected by *test cases* testing any other value from that *partition*
  + so one test for each *partition* is sufficient

***equivalence partitions***
* **can be identified for any *data* element related to the *test object***, including
  + inputs,
  + outputs,
  + configuration items,
  + internal values,
  + time-related values,
  + interface parameters
* the *partitions* may be
  + **continuous / discrete**,
  + **ordered / unordered**,
  + **finite / infinite**
* the *partitions* **must not overlap** & **must be non-empty sets**

for simple *test objects* EP can be easy,
* but **in practice**, understanding how the *test object* will treat different values is **often complicated**
* so *partitioning* should be done with care

**valid vs invalid *partitions***
* **a valid *partition***: a partition containing valid values
* **an invalid *partition***: a *partition* containing invalid values
* **definitions** of valid and invalid values **may vary among teams & organizations**
  + eg. **valid values** may be interpreted as
    - those that should be processed by the *test object* or
    - those for which the specification defines their processing
  + eg. **invalid values** may be interpreted
    - those that should be ignored / rejected by the *test object* or
    - those for which no processing is defined in the *test object* specification

in EP **the *coverage items* are the *equivalence partitions***
* **to achieve 100% *coverage*** with this technique,
  + *test cases* must exercise all identified *partitions* (including invalid *partitions*)
    - by covering each *partition* at least once
* ***coverage*** is
  + measured as
    - the number of *partitions* exercised by at least one *test case*, and
    - divided by the total number of identified *partitions*,
  + expressed as a percentage

many ***test objects* include multiple sets of *partitions***
* eg. *test objects* with more than one input parameter,
  + which means that a *test case* will cover *partitions* from different sets of *partitions*
* **Each Choice *coverage***:
  + is the simplest *coverage* criterion in the case of multiple sets of *partitions*
  + requires *test cases* to exercise each *partition* from each set of *partitions* at least once
  + does not take into account combinations of *partitions*

## 4.2.2 Boundary Value Analysis

***Boundary Value Analysis*, aka BVA** def.
* a *technique* **based on exercising the boundaries of *equivalence partitions***
* therefore, BVA **can only be used for ordered *partitions***
  + the minimum & maximum values of a *partition* are its boundary values
  + in BVA, if two elements belong to the same *partition*, all elements between them must also belong to that *partition*

BVA **focuses on the boundary values** of the *partitions*
* **because** developers are more likely to make *errors* with these boundary values
* **typical *defects*** found by BVA are **located where**
  + implemented boundaries are misplaced to positions above / below their intended positions
  + or are omitted altogether

here, we cover **two versions of the BVA**:
* 2-value & 3-value BVA
* they **differ in terms of *coverage items* per boundary** that need to be exercised to achieve 100% *coverage*

**in 2-value BVA**
* **for each boundary value there are two *coverage items***:
  + this boundary value & its closest neighbor belonging to the adjacent *partition*
* **to achieve 100% *coverage*** with 2-value BVA,
  + *test cases* must exercise all *coverage items*,
  + ie. all identified boundary values
* ***coverage*** is:
  + measured as the number of boundary values that were exercised,
    - divided by the total number of identified boundary values
  + expressed as a percentage

**in 3-value BVA**
* **for each boundary value there are three *coverage items***:
  + this boundary value and
  + both its neighbors
* therefore, in 3-value BVA some of the *coverage items* may not be boundary values
* **to achieve 100% *coverage*** with 3-value BVA,
  + *test cases* must exercise all *coverage items*,
  + ie. identified boundary values & their neighbors
* ***coverage*** is
  + measured as the number of boundary values & their neighbors exercised,
    - divided by the total number of identified boundary values & their neighbors,
  + expressed as a percentage

**3-value BVA is more rigorous than 2-value BVA**
* as it **may detect *defects* overlooked by 2-value BVA**
  + eg. if the decision `if (x ≤ 10) ...` is incorrectly implemented as `if (x = 10) ...`,
    - no *test data* derived from the 2-value BVA `x = 10, x = 11` can detect the *defect*
    - however, `x = 9`, derived from the 3-value BVA, is likely to detect it

## 4.2.3 Decision Table Testing

**decision tables**
* **used for testing the implementation of system requirements**
  + that specify how **different combinations of conditions result in different outcomes**
* an **effective way of recording complex logic**
  + eg. business rules

when creating decision tables,
* the **conditions** & the **resulting actions** of the system are defined
  + these form the **rows** of the table
  + each **column** corresponds to a decision rule that
    - defines a unique combination of conditions, along with the associated actions
* in **limited-entry decision tables**
  + all the values of the conditions & actions are shown as **Boolean** values
  + (except for irrelevant / infeasible ones, see below)
* in **extended-entry decision tables**
  + some / all the conditions & actions may also take on **multiple values**, eg.:
    - ranges of numbers,
    - *equivalence partitions*,
    - discrete values

**notation**
* for conditions:
  + **“T”**: the condition is satisfied
  + **“F”**: the condition is not satisfied
  + **“–”**: the value of the condition is irrelevant for the action outcome
  + **“N/A”**: the condition is infeasible for a given rule
* for actions:
  + **“X”**: the action should occur
  + **blank**: the action should not occur
  + **other** notations may also be used

**a full decision table**
* **has enough columns to cover every combination** of conditions
* it **can be simplified**
  + by deleting columns containing infeasible combinations of conditions
* it **can also be minimized**
  + by merging columns, in which some conditions do not affect the outcome, into a single column
  + decision table minimization algorithms are out of scope of this syllabus

in decision table testing, the ***coverage items* are the columns containing feasible combinations** of conditions
* **to achieve 100% *coverage*** with this technique,
  + *test cases* must exercise all these columns
* ***coverage*** is
  + measured as the number of exercised columns,
    - divided by the total number of feasible columns
  + expressed as a percentage

**the strength of decision table testing**
* it provides a **systematic approach to identify all the combinations** of conditions,
  + some of which might otherwise be overlooked
* it also **helps to find any gaps / contradictions** in the requirements
* **if there are many conditions**,
  + exercising all the decision rules may be time consuming,
    - since the number of rules grows exponentially with the number of conditions
  + **if so**, to reduce the number of rules that need to be exercised,
    - a **minimized** decision table / a ***risk-based*** approach may be used

## 4.2.4 State Transition Testing

**a state transition diagram**
* **models the behavior** of a system by showing its **possible states** & **valid state transitions**
* a transition is initiated by an **event**, which may be additionally qualified by a **guard condition**
  + transitions are assumed to be **instantaneous** & may sometimes **result in** the software taking action
* common **transition labeling syntax**:
  + `event [guard condition] / action`
* guard conditions & actions can be omitted
  + if they do not exist / are irrelevant for the tester

**a state table**
* a model equivalent to a state transition diagram
  + its **rows** represent states,
  + its **columns** represent events (together with guard conditions if they exist)
  + table **entries** (cells)
    - represent transitions,
    - contain the target state,
    - contain the resulting actions, if defined
* in contrast to the state transition diagram, the state table **explicitly shows invalid transitions**,
  + represented by empty cells

**a *test case*** based on a state transition diagram / state table
* is usually **represented as a sequence of events**,
* which **results in a sequence of state changes** (& **actions**, if needed)
* one *test case* may, & usually will, **cover several transitions between states**

here, we discusse 3 *coverage* criteria of the many, for state transition testing:

in **all states *coverage***,
* **the *coverage items* are the states**
* **to achieve 100% all states *coverage***, *test cases* must ensure that all the states are visited
* ***coverage*** is
  + measured as the number of visited states divided by the total number of states
  + expressed as a percentage

in **valid transitions *coverage***, aka 0-switch *coverage*
* **the *coverage items* are single valid transitions**
* **to achieve 100% valid transitions *coverage***, *test cases* must exercise all the valid transitions
* ***coverage*** is
  + measured as the number of exercised valid transitions divided by the total number of valid transitions
  + expressed as a percentage

in **all transitions *coverage***
* **the *coverage items* are all the transitions shown in a state table**
* **to achieve 100% all transitions *coverage***,
  + *test cases* must exercise all the valid transitions and attempt to execute invalid transitions
  + testing only one invalid transition in a single *test case* helps to avoid fault masking,
    - ie. a situation in which one defect prevents the detection of another
* ***coverage*** is
  + measured as the number of valid and invalid transitions exercised or attempted to be covered by executed *test cases*, divided by the total number of valid and invalid transitions
  + expressed as a percentage

use:
* **all states *coverage* is weaker** than valid transitions *coverage*,
  + because it can typically be achieved without exercising all the transitions
* **valid transitions *coverage* is the most widely used** *coverage* criterion
  + achieving full valid transitions *coverage* guarantees full all states *coverage*
  + achieving full all transitions *coverage*
    - guarantees both full all states *coverage* & full valid transitions *coverage*
    - should be a minimum requirement for mission & safety-critical software
