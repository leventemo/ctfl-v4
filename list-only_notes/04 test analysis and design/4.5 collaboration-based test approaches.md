# 4.5 Collaboration-based Test Approaches

each of the **above-mentioned techniques** in 4.2-4.4
* has a particular objective with respect to *defect* detection

***collaboration-based approaches*** on top of that
* also focus on *defect* avoidance by collaboration & communication

## 4.5.1 Collaborative User Story Writing

**a user story** def.
* represents a feature that will be valuable to either a user / purchaser of a system / software
* **the 3 C's**: user stories have three critical aspects:
  + **card**
    - the medium describing a user story
    - eg. an index card, an entry in an electronic board
  + **conversation**
    - explains how the software will be used
    - can be documented / verbal
  + **confirmation**
    - the *acceptance criteria*

**the most common format** for a user story:
* “As a [role], I want [goal to be accomplished], so that I can [resulting business value for the role]”,
* followed by the *acceptance criteria*

**collaborative authorship of the user story**
* can use techniques such as brainstorming & mind mapping
* the collaboration allows the team to obtain a shared vision of what should be delivered,
  + by taking into account three perspectives:
    - business,
    - development
    - testing

**good user stories should be:**
* **"INVEST"**:
  + independent,
  + negotiable,
  + valuable,
  + estimable,
  + small
  + testable
* **if a stakeholder does not know how to test** a user story, this may indicate that
  + the user story is not clear enough,
  + it does not reflect something valuable to them,
  + the stakeholder just needs help in testing

## 4.5.2 Acceptance Criteria

***acceptance criteria* for a user story are**
* the conditions that an implementation of the user story must meet to be accepted by stakeholders
  + so *acceptance criteria* may be viewed as the **test conditions** that should be exercised by the tests
* usually a result of the "conversation", see above

***acceptance criteria* are used to:**
* define the scope of the user story
* reach consensus among the stakeholders
* describe both positive & negative scenarios
* serve as a basis for the user story *acceptance testing*
* allow accurate planning & estimation

**two** most common **formats of writing *acceptance criteria*** for a user story:
* **scenario-oriented**
  + eg. `Given/When/Then` format used in BDD
* **rule-oriented**
  + eg. bullet point verification list / tabulated form of input-output mapping

most *acceptance criteria* can be **documented** in one of these two formats
* **however**, the team may use a custom format,
  + as long as the *acceptance criteria* are well-defined & unambiguous

## 4.5.3 Acceptance Test-driven Development (ATDD)

**ATDD**:
* **a test-first approach**
* ***test cases* are created**
  + **prior to implementing** the user story
  + by team members **with different perspectives**
    - eg. customers, developers, and testers
* *test cases* may be **executed manually / automated**

**step1: a specification workshop** where
* the **user story &** (if not yet defined) its ***acceptance criteria*** are
  + **analyzed**, **discussed** & **written** by the team members
* **incompleteness, ambiguities / *defects*** in the user story are **resolved**

**step2: create the *test cases***
* can be done **by the team** as a whole **/ by the tester** individually
* ***test cases* are based on the *acceptance criteria***
  + & can be seen as examples of how the software works
    - since examples & tests are the same, these terms are often used interchangeably
  + this will help the team implement the user story correctly
* during the *test design* the *test techniques* described in 4.2–4.4 may be applied

**the *test design* process:**
* **typically, the first *test cases* are positive**,
  + confirming the correct behavior without exceptions / error conditions,
  + & comprising the sequence of activities executed if everything goes as expected
* after the positive *test cases* are done, the **team should perform negative testing**
* **finally**, the team should cover **non-functional quality characteristics** as well
  + eg. performance efficiency, usability

***test cases* should be** expressed in a way that is **understandable for the stakeholders**
* typically, *test cases* **contain sentences in natural language** involving
  + the necessary **preconditions** (if any),
  + the **inputs**,
  + the **postconditions**

**the scope of *test cases***
* they **must cover all the characteristics of the user story**
* they **should not go beyond the story**
* **however, the *acceptance criteria* may detail some** of the issues described in the user story
* in addition, **no two *test cases* should describe the same characteristics** of the user story

when captured **in a format supported by a test automation framework**,
* the developers can automate the *test cases*
  + by writing the supporting code as they implement the feature described by a user story
* the acceptance tests then become executable requirements
