# 5.1 Test Planning

## 5.1.1 Purpose & Content of a Test Plan

**a *test plan*: what it does**
* **describes**
  + the **objectives**, **resources** & **processes** for a test project
* **documents**
  + the **means** & **schedule** for achieving *test objectives*
* **helps to ensure** that
  + the performed **test activities** will **meet the established criteria**
* serves as
  + **a means of communication** with team members & other stakeholders
* **demonstrates** that
  + **testing will adhere to** the existing **test policy** & **test strategy**
  + or explains **why** the testing will **deviate** from them

***test planning***
* **guides** the tester's thinking
* **forces** testers to confront the future challenges related to
  + *risks*, schedules, people, tools, costs, effort, etc.
* the process of preparing a *test plan* is **a useful way**
  + **to think through** the efforts needed to achieve the *test* project *objectives*

the **typical content of a *test plan*** includes:
* **context** of testing
  + eg. scope, *test objectives*, constraints, *test basis*
* **assumptions & constraints** of the test project
* **stakeholders**
  + eg. roles, responsibilities, relevance to testing, hiring & training needs
* **communication**
  + eg. forms & frequency of communication, documentation templates
* ***risk* register**
  + eg. *product risks*, *project risks*
* ***test approach***, eg.
  + *test levels*,
  + *test types*,
  + *test techniques*,
  + test deliverables,
  + *entry criteria* & *exit criteria*,
  + independence of testing,
  + metrics to be collected,
  + *test data* requirements,
  + test environment requirements,
  + deviations from the organizational test policy & test strategy
* **budget & schedule**

more details about *test plan* & its content: ISO/IEC/IEEE 29119-3

## 5.1.2 Tester's Contribution to Iteration & Release Planning

**in iterative SDLCs**, typically 2 kinds of planning occur:
* **release planning**
* **iteration planning**

**release planning**
* **looks ahead to the release of a product**,
  + defines & re-defines the **product backlog**,
  + may involve refining larger user stories into a set of smaller user stories
* it also serves as the basis for the *test approach* & *test plan* **across all iterations**
* **testers involved in release planning**
  + participate in writing testable user stories & *acceptance criteria*,
  + participate in project & quality *risk analyses*,
  + estimate test effort associated with user stories,
  + determine the *test approach*,
  + plan the testing for the release

**iteration planning**
* **looks ahead to the end of a single iteration**
  + is concerned with the **iteration backlog**
* **testers involved in iteration planning**
  + participate in the detailed *risk analysis* of user stories,
  + determine the testability of user stories,
  + break down user stories into tasks (particularly testing tasks),
  + estimate test effort for all testing tasks,
  + identify & refine *functional* & *non-functional* aspects of the *test object*

## 5.1.3 Entry Criteria & Exit Criteria

***entry criteria*** def.
* define the preconditions for undertaking a given activity
* if *entry criteria* are not met,
  + it is likely that the activity will prove to be more difficult, time-consuming, costly, & riskier

***exit criteria***
* define what must be achieved in order to declare an activity completed

***entry criteria* & *exit criteria***
* should be defined for each *test level*,
* will differ based on the *test objectives*

**typical *entry criteria*** include:
* **availability of resources**
  + eg. people, tools, environments, *test data*, budget, time
* **availability of testware**
  + eg. *test basis*, testable requirements, user stories, *test cases*,
* **initial quality level of a *test object***
  + eg. all smoke tests have passed

**typical *exit criteria*** include:
* **measures of thoroughness**, eg.
  + achieved level of *coverage*,
  + number of unresolved *defects*,
  + *defect* density,
  + number of failed *test cases*
* ***completion criteria***, eg.
  + planned tests have been executed,
  + *static testing* has been performed,
  + all *defects* found are reported,
  + all *regression tests* are automated

**running out of time / budget**
* **can also be viewed as valid *exit criteria***
* even without other *exit criteria* being satisfied,
  + it can be acceptable to end testing under such circumstances,
  + **if stakeholders have reviewed & accepted the *risk*** to go live without further testing

**in Agile** software development
* *exit criteria* are often called **Definition of Done**,
  + defining the team’s objective metrics for a releasable item
* *entry criteria* are called **Definition of Ready**
  + criteria that a user story must fulfill to start the development and/or testing activities

## 5.1.4 Estimation Techniques

**test effort estimation involves**
* **predicting the amount of test-related work needed** to meet the objectives of a test project
* it is important to make it clear to the stakeholders
  + that the estimate is **based on a number of assumptions** & is **always subject to estimation error**
* estimation **for small tasks is usually more accurate** than for the large ones
  + so a large task can be **decomposed into a set of smaller tasks**
    - which then in turn can be estimated

**4 estimation techniques** are described in this syllabus:

**estimation based on ratios**
* a **metrics-based technique**
* **figures are collected from previous projects** within the organization,
  + which makes it possible to **derive “standard” ratios** for similar projects
* the ratios of an organization’s own projects are **generally the best source** to use in the estimation process
  + eg. taken from historical data
* these standard ratios can then be used to estimate the test effort for the new project, **eg**.
  + if in the previous project the dev-to-test effort ratio was 3:2,
  + & in the current project the dev effort is expected to be 600 person-days,
  + the test effort can be estimated to be 400 person-days

**extrapolation**
* a **metrics-based technique**,
* **measurements are made as early as possible** in the current project to gather the data
* having **enough observations**,
  + the effort required for the remaining work can be **approximated by extrapolating this data**
    - usually by applying a mathematical model
* this method is **very suitable in iterative SDLCs**
  + **eg**. the team may extrapolate the test effort in the forthcoming iteration
    - as the averaged effort from the last three iterations

**Wideband Delphi**
* an iterative, **expert-based technique**, experts make experience-based estimations
  + **each expert, in isolation**, estimates the effort
  + **results are collected**
    - if there are deviations, out of range of the agreed upon boundaries,
    - experts discuss their current estimates
  + each expert is then asked to make a **new estimation based on that feedback**, again in isolation
  + this process is **repeated until a consensus is reached**
* **Planning Poker**: a variant of Wideband Delphi, commonly used in Agile software development
  + estimates are usually made using cards with numbers that represent the effort size

**three-point estimation**
* an **expert-based technique**
* **three estimations are made** by the experts:
  + the **most optimistic** estimation (a),
  + the **most likely** estimation (m)
  + the **most pessimistic** estimation (b)
* the final estimate (E) is their **weighted arithmetic mean**
* the **most popular version of this technique:**
  + the estimate is calculated as `E = (a + 4*m + b) / 6`
  + the advantage of this technique:
    - allows the experts to calculate the measurement error: `SD = (b – a) / 6`
    - eg. if the estimates (in person-hours) are: `a=6`, `m=9` and `b=18`,
    - the final estimation is `10±2` person-hours
    - ie. between 8 & 12 person-hours,
    - because `E = (6 + 4*9 + 18) / 6 = 10` & `SD = (18 – 6) / 6 = 2`

## 5.1.5 Test Case Prioritization

once the *test cases* & test procedures are specified & assembled into test suites,
* these test suites can be arranged in a **test execution schedule**
  + that **defines the order** in which they are to be run
* when prioritizing *test cases*, **different factors can be taken into account**

most commonly used ***test case* prioritization strategies**:
* ***risk-based* prioritization**
  + the order of test execution is based on the results of *risk analysis*
  + *test cases* covering the most important *risks* are executed first
* **coverage-based prioritization**
  + the order of test execution is based on *coverage* (eg. *statement coverage*)
  + *test cases* achieving the highest *coverage* are executed first
  + in another variant, called **additional *coverage* prioritization**,
    - the *test case* achieving the highest *coverage* is executed first
    - each subsequent *test case* is the one that achieves the highest additional *coverage*
* **requirements-based prioritization**,
  + the order of test execution is based on the priorities of the requirements traced back to the corresponding *test cases*
  + requirement priorities are defined by stakeholders
  + *test cases* related to the most important requirements are executed first

**ideally, *test cases* would be ordered to run based on their priority levels**,
* using, eg. one of the above-mentioned prioritization strategies
* **however**, this practice may not work if the *test cases* / the features being tested have **dependencies**
  + if a *test case* with a higher priority is dependent on a *test case* with a lower priority,
  + the lower priority *test case* must be executed first

the order of test execution must **also take into account the availability of resources**, eg.
* the required test tools,
* test environments,
* people that may only be available for a specific time window

## 5.1.6 Test Pyramid

the *test pyramid*
* is a model showing that different tests may have different **granularity**
* it supports the team in *test automation* & test effort allocation
  + by showing that **different goals are supported by different levels of *test automation***
* the **pyramid layers represent groups of tests**
  + **the higher the layer**, the lower the test granularity, test isolastion & test execution time
  + tests in the **bottom layer** are small, isolated, fast & check a small piece of functionality,
    - so usually a lot of them are needed to achieve a reasonable *coverage*
  + the **top layer** represents complex, high-level, end-to-end tests
    - these high-level tests are generally slower than the tests from the lower layers,
    - they typically check a large piece of functionality,
    - so usually just a few of them are needed to achieve a reasonable *coverage*
* the **number & naming of the layers may differ**
  + the original *test pyramid* model (Cohn 2009) defines three layers:
    - unit tests,
    - service tests,
    - UI tests
  + another popular model defines:
    - unit (component) tests,
    - integration (component integration) tests,
    - end-to-end tests
  + other *test levels* can also be used

## 5.1.7 Testing Quadrants

the *testing quadrants*,
* group the *test levels* with the appropriate *test types*, activities, *test techniques* & work products in the Agile software development
* the model supports *test management* in
  + visualizing these to ensure that all appropriate *test types* & *test levels* are included in the SDLC
  + understanding that some *test types* are more relevant to certain *test levels* than others
* also provides a way to differentiate & describe the types of tests to all stakeholders,
  + including developers, testers, & business representatives

in this model,
* tests can be
  + **business facing**
  + **or technology facing**
* tests can also
  + **support the team**
    - ie. guide the development
  + **or critique the product**
    - ie. measure its behavior against the expectations

the combination of these two viewpoints determines **the four quadrants**:
* **Quadrant Q1**:
  + **technology facing, support the team**
  + contains component & component integration tests
  + these tests should be automated & included in the CI process
* **Quadrant Q2**:
  + **business facing, support the team**
  + contains *functional* tests, examples, user story tests, user experience prototypes, API testing, & simulations
  + these tests check the *acceptance criteria* & can be manual / automated
* **Quadrant Q3**:
  + **business facing, critique the product**
  + contains exploratory testing, usability testing, *user acceptance testing*
  + these tests are user-oriented & often manual
* **Quadrant Q4**:
  + **technology facing, critique the product**
  + contains smoke tests & non-functional tests (except usability tests)
  + these tests are often automated
