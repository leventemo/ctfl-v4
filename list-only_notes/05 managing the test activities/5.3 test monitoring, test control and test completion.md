# 5.3 Test Monitoring, Test Control & Test Completion

***test monitoring***
* is **concerned with gathering information** about testing
* this **info is used**
  + to **assess *test progress***
  + to **measure if** the test ***exit criteria*** / the **test tasks** associated with the *exit criteria* **are satisfied**, eg.
    - meeting the targets for *coverage* of *product risks*,
    - requirements,
    - *acceptance criteria*

***test control***
* uses the **info from *test monitoring* to provide**,
  + in a form of
    - the **control directives**,
    - **guidance & the necessary corrective actions**
  + to achieve the most effective & efficient testing
* **examples of control directives** include:
  + reprioritizing tests when an identified *risk* becomes an issue
  + re-evaluating whether a *test item* meets *entry criteria* / *exit criteria* due to rework
  + adjusting the test schedule to address a delay in the delivery of the test environment
  + adding new resources when & where needed

***test completion***
* **collects data from completed test activities**
  + to consolidate experience, testware & any other relevant information
* *test completion* activities **occur at project milestones**, eg.
  + when a *test level* is completed,
  + an agile iteration is finished,
  + a test project is completed (or cancelled),
  + a software system is released,
  + a maintenance release is completed

## 5.3.1 Metrics used in Testing

**test metrics**
* are gathered **to show progress against**
  + the planned schedule & budget,
  + the current quality of the *test object*,
  + the effectiveness of the test activities with respect to the objectives / an iteration goal
* *test monitoring* gathers a variety of metrics to support the *test control* & *test completion*

**common test metrics include**:
* **project progress metrics**, eg.
  + task completion,
  + resource usage,
  + test effort
* **test progress metrics**, eg.
  + *test case* implementation progress,
  + test environment preparation progress,
  + number of *test cases* run/not run,
  + passed/failed,
  + test execution time
* **product quality metrics**, eg.
  + availability,
  + response time,
  + mean time to *failure*
* ***defect* metrics**, eg.
  + number & priorities of *defects* found/fixed,
  + *defect* density,
  + *defect* detection percentage
* ***risk* metrics**, eg.
  + residual *risk level*
* ***coverage* metrics**, eg.
  + requirements *coverage*,
  + code *coverage*
* **cost metrics**, eg.
  + cost of testing,
  + organizational cost of quality

## 5.3.2 Purpose, Content & Audience for Test Reports

**test reporting**
* **summarizes** & **communicates** test information **during** & **after** testing
* ***test progress reports***
  + support the ongoing control of the testing
  + must provide enough information to make modifications to
    - the test schedule, resources / *test plan*,
  + when such changes are needed due to deviation from the plan / changed circumstances
* ***test completion reports***
  + summarize a specific stage of testing
    - eg. *test level*, test cycle, iteration
  + & can give information for subsequent testing

***test progress reports***
* generated by the test team
  + for stakeholders to keep them informed
  + during *test monitoring* & *control*
  + usually generated on a regular basis
    - eg. daily, weekly, etc.
* **typical *test progress reports* include**:
  + test **period**
  + test **progress**
    - eg. ahead / behind schedule, including any notable deviations
  + **impediments** for testing & their **workarounds**
  + test **metrics**
    - see eg-s in 5.3.1 above
  + new & changed ***risks*** within testing period
  + **testing planned** for the next period

a ***test completion report***
* prepared **during *test completion***
  + when a project, *test level* / *test type* is complete
  + & when, ideally, its *exit criteria* have been met
* this report **uses *test progress reports* & other *data***
* **typical *test completion reports* include**:
  + test **summary**
  + testing & product **quality evaluation** based on the original *test plan*
    - ie. *test objectives* & *exit criteria*
  + **deviations** from the *test plan*
    - eg. differences from the planned schedule, duration & effort
  + testing **impediments** & **workarounds**
  + test **metrics** based on *test progress reports*
  + **unmitigated *risks***, ***defects* not fixed**
  + **lessons learned** that are relevant to the testing

**different audiences**
* require **different information** in the reports,
* influence
  + the degree of **formality**,
  + the **frequency** of reporting
* eg. reporting on test progress to others in the same team
  + is often frequent & informal
* eg. reporting on testing for a completed project
  + follows a set template
  + occurs only once

ISO/IEC/IEEE 29119-3 standard includes
* templates & examples for
  + *test progress reports*, called test status reports,
  + *test completion reports*

## 5.3.3 Communicating the Status of Testing

the best **means of communicating test status varies**, depending on
  + *test management* concerns,
  + organizational test strategies,
  + regulatory standards,
  + or, in the case of self-organizing teams (see section 1.5.2), on the team itself
* the **options include**:
  + **verbal communication** with team members & other stakeholders
  + **dashboards**
    - eg. CI/CD dashboards, task boards & burn-down charts
  + electronic **communication channels**
    - eg. email, chat
  + online **documentation**
  + **formal test reports**
    - see section 5.3.2 above

**one / more** of these options can be used
* **more formal** communication may be more appropriate for
  + **distributed teams** where direct face-to-face communication is not always possible
    - due to geographical distance / time differences
* typically, **different stakeholders** are interested in **different types of information**,
  + so communication should be **tailored accordingly**
